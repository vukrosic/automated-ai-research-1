Of course. Here is a concise research plan based on the provided MoE implementation and constraints.

### **Research Plan: The Role of Routing Noise in Small-Scale Mixture-of-Experts Models**

#### **1. Research Question & Novel Contribution**

**Research Question:** What is the impact of adding stochastic noise to router logits on the training stability, final performance, and expert utilization of a *small-scale* Mixture-of-Experts (MoE) language model?

**Background & Hypothesis:** In large-scale MoE models (e.g., Mixtral), adding noise to the router logits during training is a common technique. This encourages exploration, preventing early "expert collapse" where the router exclusively sends tokens to a few preferred experts. However, it is an open question whether this technique is beneficial, neutral, or even detrimental in a small-model, small-dataset regime like the one presented.

**Novel Contribution:** This research will be the first to specifically investigate the utility of routing noise at this small scale (384-dimension model on the Smollm dataset). Our contribution will be to determine if this technique, designed for massive models, provides tangible benefits (better performance, more balanced expert load) or if it simply adds unhelpful randomness that hinders learning in a resource-constrained setting.

**Hypothesis:** We hypothesize that for a small model and a relatively limited dataset, deterministic routing (no noise) will achieve comparable or even slightly better performance than noisy routing. The rationale is that with fewer parameters and less complex data, the model may not need forced exploration to find an effective token-to-expert mapping, and the added noise might disrupt an already fragile learning process.

---

#### **2. Experimental Design & Fairness**

To answer our research question, we will conduct a controlled experiment with three conditions, varying only the standard deviation of the Gaussian noise added to the router logits.

**Control Group (Baseline):**
*   **Condition A:** `noise_std = 0.1`. This is the implementation as provided in the code, representing the standard approach of using mild noise.

**Experimental Groups:**
*   **Condition B:** `noise_std = 0.0`. This creates a fully deterministic router, testing the necessity of noise.
*   **Condition C:** `noise_std = 0.5`. This uses high noise to test if excessive exploration is harmful, potentially preventing the model from learning stable routing policies.

**Implementation of Fairness:**
To ensure a fair and rigorous comparison, the following measures will be strictly enforced:

1.  **Constant Hyperparameters:** All model and training parameters (`d_model`, `n_layers`, `max_steps`, `batch_size`, `muon_lr`, `load_balancing_weight`, etc.) will remain identical across all three experimental runs.
2.  **Fixed Seed:** The global seed will be set to `42` for all experiments (`set_seed(42)`). This ensures identical model initializations and data shuffling, making results directly comparable.
3.  **Identical Data Splits:** The `torch.utils.data.random_split` uses a generator with a fixed manual seed, guaranteeing that the train and validation datasets are exactly the same for every run.
4.  **Consistent Environment:** All experiments will be executed on the same single NVIDIA RTX 4090 GPU to eliminate variance from hardware differences.
5.  **Parameter Count:** The model architecture is not changed between experiments, ensuring the total and active parameter counts are identical. The only change is a single float value (`noise_std`) that does not affect model size.

---

#### **3. Methodological Issues in Existing Code**

The provided code is a strong starting point, but we must acknowledge a potential issue affecting fairness if not handled carefully:

*   **Inconsistent Evaluation Trigger:** In the `main` block, the config is initialized with `eval_every=10000000`, but the training loop has a check `if step % config.eval_every == 0`. This means the intermediate evaluation will never run. The only evaluation performed is the *final* one after the training loop completes.
    *   **Mitigation:** For this short experiment, this is acceptable as we are primarily interested in the final converged performance. We will rely solely on the final evaluation metrics for our comparison, ensuring fairness. For a longer study, `eval_every` should be set to a reasonable value (e.g., 500) to monitor training progress.

---

#### **4. Success Metrics**

We will evaluate each condition based on a combination of performance and expert utilization metrics:

1.  **Primary Metrics (Model Performance):**
    *   **Final Validation Perplexity:** The primary indicator of language modeling capability. Lower is better.
    *   **Final Validation Loss:** The raw cross-entropy loss on the validation set. Lower is better.
    *   **Final Validation Accuracy:** The token-level prediction accuracy. Higher is better.

2.  **Secondary Metrics (Expert Utilization):**
    *   **Load Balancing Loss (`aux_loss`):** The value of the auxiliary loss during training. We will log its average value over the last 100 steps. A lower value indicates the router is naturally balancing the load, which is a desirable property.
    *   **Expert Usage Distribution (Post-Hoc):** After training, we will run one validation batch through the model and count the number of tokens assigned to each of the 8 experts. We will report the **coefficient of variation** (std dev / mean) of these counts. A lower coefficient indicates more balanced expert usage.

A successful outcome for our hypothesis would be Condition B (no noise) showing the best or statistically indistinguishable primary metrics compared to Condition A, while also maintaining a healthy (low coefficient of variation) expert usage distribution.

---

#### **5. Timeline (Total: ~4 Hours)**

*   **Phase 1: Baseline & Code Modification (1 hour):**
    *   Run the baseline experiment (Condition A) to confirm runtime (expected: ~5-7 minutes) and establish performance benchmarks.
    *   Modify the `TopKRouter` to accept `noise_std` as an argument. Add logging for the coefficient of variation metric in the final evaluation step.

*   **Phase 2: Experiment Execution (1 hour):**
    *   Run experiments for Condition B (`noise_std=0.0`) and Condition C (`noise_std=0.5`).
    *   Record all primary and secondary metrics for each of the three runs.

*   **Phase 3: Analysis & Reporting (2 hours):**
    *   Aggregate results into a table comparing the three conditions across all metrics.
    *   Create plots to visualize the final expert usage distribution for each condition.
    *   Draw conclusions and summarize findings.