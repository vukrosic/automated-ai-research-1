Here is the final, concise research plan, revised to incorporate the critical feedback while adhering to all system constraints.

### **Final Research Plan: A Rigorous Re-evaluation of Routing Noise in Small-Scale MoEs**

This plan outlines a focused, fair, and feasible experiment to determine the true impact of routing noise in a small-scale MoE model, controlling for key confounding variables as identified in the critique.

#### **1. Research Question & Hypothesis**

**Research Question:** In a small-scale MoE model (17M params) trained on a limited dataset (Smollm), does stochastic routing noise improve performance, and is this effect independent of the auxiliary load-balancing loss?

**Novel Contribution:** While routing noise is standard practice in large models, its utility in a resource-constrained regime is unclear. This work provides the first controlled, multi-seed evaluation to disentangle the effects of routing noise from the confounding influence of the load-balancing penalty, specifically on the Smollm dataset. We will provide a robust, reproducible answer for this specific small-scale context.

**Hypothesis:** We hypothesize that deterministic routing (`noise_std = 0.0`) will achieve superior performance (lower perplexity) compared to noisy routing, *especially* when the load-balancing regularizer is removed (`load_balancing_weight = 0.0`). We suspect the noise adds unhelpful randomness that harms learning, and the load-balancing weight may unfairly penalize the deterministic model, which can learn to balance tokens without forced exploration.

---

#### **2. Experimental Design**

To isolate the variable of interest and test our hypothesis, we will use a 3x2 factorial design with replications.

**Independent Variables:**
1.  **Routing Noise (`noise_std`):**
    *   `0.0` (Deterministic Routing)
    *   `0.1` (Low Noise - Baseline)
    *   `0.5` (High Noise - Stress Test)
2.  **Load Balancing Weight (`load_balancing_weight`):**
    *   `1e-2` (Standard Value - Confounder Present)
    *   `0.0` (Disabled - Confounder Removed)

**Replications:** Each of the 6 conditions (3 noise levels × 2 weight levels) will be run **3 times** with different random seeds (`42`, `43`, `44`).

**Total Experiments:** 6 conditions × 3 seeds = **18 runs**.
*   Expected Runtime: ~6 minutes/run × 18 runs ≈ 1.8 hours.

---

#### **3. Implementation of Rigor and Fairness**

This design incorporates the critique's feedback to ensure the results are robust and scientifically valid.

1.  **Multiple Seeds:** By running each condition three times, we can calculate `mean` and `standard deviation` for all metrics, allowing us to assess if observed differences are statistically meaningful or simply due to random initialization.
2.  **Confounder Control:** The factorial design explicitly tests the interaction between noise and the load-balancing weight, preventing us from misattributing performance changes to the wrong cause.
3.  **Robust Evaluation:** The evaluation will be run every `max_steps // 10` steps (i.e., every 500 steps). We will report the **best validation perplexity** achieved across all evaluations during training to mitigate bias from a single lucky or unlucky final step.
4.  **Stable Metrics:** The expert utilization metric (Coefficient of Variation) will be calculated on the **entire validation set** at the end of training, not on a single batch, to provide a stable, reliable measurement.
5.  **Strictly Constant Hyperparameters:** All other parameters (`d_model`, `n_layers`, `max_steps`, `batch_size`, `muon_lr`, etc.) will be identical across all 18 runs. The global seed for each run will ensure identical data splits and model initializations for that run.
6.  **Resource Guardrails:** Before each training run, we will print the total and active parameter counts to confirm the model size is constant (17M / 2.3M). We will also log the total runtime and max GPU memory usage for each job to verify it stays within the 2-10 minute constraint.

---

#### **4. Success Metrics & Criteria**

For each of the 6 conditions, we will report the **mean ± standard deviation** across the 3 seeds.

**Primary Metric (Model Performance):**
*   **Best Validation Perplexity:** The primary indicator of model quality. Lower is better.

**Secondary Metrics:**
*   **Best Validation Loss & Accuracy:** To supplement perplexity.
*   **Final Auxiliary Loss (for `weight=1e-2` runs):** Indicates how well the router balanced the load.
*   **Expert Usage Coefficient of Variation (CV):** Measures the balance of token allocation across experts post-training. A lower CV is better.

**Success Criteria:** Our hypothesis will be confirmed if the `noise_std=0.0` condition shows a statistically superior (e.g., non-overlapping standard error bars) mean perplexity compared to the `noise_std=0.1` and `noise_std=0.5` conditions, particularly in the `load_balancing_weight=0.0` setting. If the performance ranking flips between the `weight=1e-2` and `weight=0.0` settings, it will confirm that the load-balancing term was a significant confounder.

---

#### **5. Minimal Viable Implementation & Timeline**

**Code Modifications:**
1.  Make `noise_std` and `load_balancing_weight` command-line arguments or config parameters.
2.  Wrap the main training function in a loop that iterates through seeds `[42, 43, 44]`.
3.  Change `eval_every` in the config from the default to `500`. Track the best validation perplexity seen so far during the training loop.
4.  Modify the final evaluation logic to iterate over the entire validation DataLoader to compute the expert usage CV.
5.  Add `torch.cuda.max_memory_allocated()` and `time.time()` logging.

**Timeline (Total: ~4-5 hours):**
*   **Phase 1: Code Modification & Verification (1 hour):** Implement the changes above and run a single job (`seed=42`, `noise_std=0.1`, `weight=1e-2`) to verify correctness and confirm runtime is within the 2-10 minute window.
*   **Phase 2: Experiment Execution (2 hours):** Launch all 18 runs.
*   **Phase 3: Analysis & Reporting (1-2 hours):** Aggregate results into a 3x2 table (mean ± std), plot the key comparisons, and draw final conclusions. A CSV with raw results for all 18 runs will be generated for reproducibility.