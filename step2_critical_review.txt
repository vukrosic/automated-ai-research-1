Concise critical review (≈ 420 words)

1. Unfair comparison: only one run per condition  
   With a single 42-seed run you cannot tell whether an observed 0.XX perplexity gap is caused by noise or by random initialization / data order. One outlier run can reverse the conclusion.

   Fix (still ≤ 10 min per job):  
   - Run each condition 3 times with seeds 42, 43, 44.  
   - Report mean ± std; use a paired t-test (or simply check whether CIs overlap).  
   Wall-clock cost: 3 cond × 3 runs × 7 min ≈ 63 min, still inside the “few hours” budget.

2. Statistical validity: no power analysis, tiny delta may be noise  
   A 0.05 perplexity difference is meaningless if the std across seeds is 0.08.  
   After the 3-run data are collected, compute Cohen’s d or simply require the delta to exceed 2 × SEM before claiming “better”.

3. Evaluation bias: only final-step metric, no training curve  
   The current code evaluates once at the very end. A single lucky or unlucky step can bias the result.  
   Fix: set `eval_every = max_steps//10` (≈ 100) and average the last three checkpoints (or use the best-of-three). This adds < 20 s per run.

4. Hidden confounder: load-balancing weight is arbitrary  
   `load_balancing_weight = 1e-2` is tuned for `noise_std = 0.1`. When you remove noise the auxiliary loss already drops; the fixed weight may then over-penalise the no-noise run and artifically inflate its perplexity.  
   Quick control: run an extra ablation with `weight = 0` for all three noise levels (9 extra runs, still < 1 h). If the ranking flips, the weight—not the noise—is the real cause.

5. Expert-utilisation metric is noisy  
   Counting tokens on a single batch gives a CV with ±30 % variance.  
   Use the whole validation set once; it costs < 10 s and stabilises the CV.

6. Over-stated claim: “first to study noise at small scale”  
   Routing noise in 100 M–1 B models was ablated in (Riquelme et al. 2021) and noise-free Switch-Transformer variants appear in several < 500 M parameter papers. Tone the novelty down to “we give the first controlled 3-run evaluation under 400 M tokens on SmolLM”.

7. Resource adherence is good but add explicit guardrails  
   - Log the actual minutes and max GPU memory each run consumes; stop and report if any job > 10 min / 24 GB.  
   - Guarantee the model stays at 17 M total / 2.3 M active params by printing parameter count before training.

8. Release checklist  
   - Upload a CSV with all raw run results (seed, perplexity, loss, acc, CV).  
   - Provide the exact command line / config diff so reviewers can reproduce one run in < 5 min.

Implementing 1+3+4 already removes the biggest threats to fairness without violating dataset, model-size or runtime limits.